{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaku\n",
    "**A demonstration of the goal distance and measurement framework in Kyudo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports for package management\n",
    "import os\n",
    "import json\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Module constants and fixtures\n",
    "FIXTURES    = os.path.join(os.getcwd(), 'fixtures')\n",
    "CORPUS_PATH = os.path.join(FIXTURES, 'questions.corpus.json')\n",
    "\n",
    "# Tasks Constants\n",
    "WHO    = 'who'\n",
    "WHAT   = 'WHAT'\n",
    "WHEN   = 'WHEN'\n",
    "WHERE  = 'WHERE'\n",
    "WHY    = 'WHY'\n",
    "HOW    = 'HOW'\n",
    "EXIST  = 'EXISTENTIAL'\n",
    "PERMIT = 'PERMISSION'\n",
    "UNKNWN = 'UNKNOWN'\n",
    "\n",
    "# Task Mapping\n",
    "TASK_MAP    = {\n",
    "    'who':    WHO, \n",
    "    'what':   WHAT,\n",
    "    'when':   WHEN,\n",
    "    'where':  WHERE,\n",
    "    'why':    WHY,\n",
    "    'how':    HOW,\n",
    "    'which':  WHAT,\n",
    "    'in':     WHAT,\n",
    "    'are':    EXIST,\n",
    "    'on':     WHAT,\n",
    "    'can':    PERMIT,\n",
    "    'does':   EXIST,\n",
    "    'is':     EXIST,\n",
    "    'at':     WHERE,\n",
    "    'for':    WHAT,\n",
    "    'with':   WHAT,\n",
    "    'did':    EXIST,\n",
    "    'whats':  WHAT,\n",
    "    'should': PERMIT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(corpus=CORPUS_PATH):\n",
    "    \"\"\"\n",
    "    Reads and parses corpus data and yields each item at a time.\n",
    "    \"\"\"\n",
    "    with open(corpus, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for item in data:\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Preprocesses text to ensure that it is lower cased.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, texts):\n",
    "        return [\n",
    "            text.strip().lower()\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "\n",
    "class TaskVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Provide the task feature from an utterance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, utterances):\n",
    "        return [\n",
    "            self.extract_task(utterance) \n",
    "            for utterance in utterances\n",
    "        ]\n",
    "    \n",
    "    def extract_task(self, utterance):\n",
    "        tokens = wordpunct_tokenize(utterance)\n",
    "        first  = tokens[0]\n",
    "        if first in TASK_MAP:\n",
    "            return {'task': TASK_MAP[first]}\n",
    "        \n",
    "        return {'task': UNKNWN}\n",
    "\n",
    "## Construct the Feature Pipeline\n",
    "feats = Pipeline([\n",
    "    # Preprocess text to make sure it is normalized.\n",
    "    ('preprocess', TextPreprocessor()),\n",
    "        \n",
    "    # Use FeatureUnion to combine concept, task, and context features \n",
    "    ('union', FeatureUnion(\n",
    "        \n",
    "        # Create union of TF-IDF and Task Vectorizers\n",
    "        transformer_list=[\n",
    "            \n",
    "            # Pipeline for Concept Extraction\n",
    "            ('concepts', Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=wordpunct_tokenize)),\n",
    "                ('best', TruncatedSVD(n_components=50)),\n",
    "            ])),\n",
    "                    \n",
    "            # Pipeline for Task Extraction\n",
    "            ('tasks', Pipeline([\n",
    "                ('tasks', TaskVectorizer()),\n",
    "                ('vect', DictVectorizer()),\n",
    "            ])),\n",
    "            \n",
    "                    \n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'concepts': 0.45,\n",
    "            'tasks': 0.55,\n",
    "        },\n",
    "    )),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocess', TextPreprocessor()), ('union', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('concepts', Pipeline(steps=[('tfidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowerca...ort=True,\n",
       "        sparse=True))]))],\n",
       "       transformer_weights={'tasks': 0.55, 'concepts': 0.45}))])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.fit(read_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.0852185900015\n",
      "  (0, 1)\t0.0315209099528\n",
      "  (0, 2)\t0.0568007038668\n",
      "  (0, 3)\t-0.0361139208257\n",
      "  (0, 4)\t-0.067512677812\n",
      "  (0, 5)\t0.0134246498796\n",
      "  (0, 6)\t0.0321236105732\n",
      "  (0, 7)\t-0.0250677447214\n",
      "  (0, 8)\t0.00097913443713\n",
      "  (0, 9)\t0.00561780033721\n",
      "  (0, 10)\t-0.00736342322186\n",
      "  (0, 11)\t-0.0165369384268\n",
      "  (0, 12)\t-0.00172181491667\n",
      "  (0, 13)\t0.000891893120333\n",
      "  (0, 14)\t0.0198861130849\n",
      "  (0, 15)\t-0.00829117992882\n",
      "  (0, 16)\t0.00200409416144\n",
      "  (0, 17)\t0.00400897570371\n",
      "  (0, 18)\t0.00522655472095\n",
      "  (0, 19)\t-0.0138163001452\n",
      "  (0, 20)\t-0.00253459183025\n",
      "  (0, 21)\t0.00602975391631\n",
      "  (0, 22)\t0.00310560120697\n",
      "  (0, 23)\t0.000463039075122\n",
      "  (0, 24)\t-0.00890815484898\n",
      "  :\t:\n",
      "  (0, 26)\t-0.0135424346071\n",
      "  (0, 27)\t0.00719519199843\n",
      "  (0, 28)\t-0.00435666999392\n",
      "  (0, 29)\t-0.00457773381357\n",
      "  (0, 30)\t-0.000897927855733\n",
      "  (0, 31)\t-0.012106371337\n",
      "  (0, 32)\t0.00679734992166\n",
      "  (0, 33)\t-0.00162369163835\n",
      "  (0, 34)\t0.00906422418398\n",
      "  (0, 35)\t-0.00423696426473\n",
      "  (0, 36)\t0.00564358846507\n",
      "  (0, 37)\t-0.00340856070295\n",
      "  (0, 38)\t-0.00338520267835\n",
      "  (0, 39)\t-0.00367278676985\n",
      "  (0, 40)\t0.00622964407061\n",
      "  (0, 41)\t0.0063469541167\n",
      "  (0, 42)\t0.000965350529887\n",
      "  (0, 43)\t-0.00213374033933\n",
      "  (0, 44)\t-0.00638849753402\n",
      "  (0, 45)\t0.000228867449424\n",
      "  (0, 46)\t-0.00331705972623\n",
      "  (0, 47)\t-0.00837763918853\n",
      "  (0, 48)\t-0.0106557371346\n",
      "  (0, 49)\t-0.00264646378449\n",
      "  (0, 56)\t0.55\n"
     ]
    }
   ],
   "source": [
    "# Print feature representation\n",
    "print feats.transform(('where is the food truck?',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build kNearestNeighbors clustering mechanism\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(feats.fit_transform(read_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_related_questions(question):\n",
    "    features = feats.transform((question,))\n",
    "    distances, indices = nbrs.kneighbors(features)\n",
    "    for idx, text in enumerate(read_data()):\n",
    "        if idx in indices:\n",
    "            print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are bizarre events that happen in washington dc?\n",
      "what are bizarre things that people eat in washington dc?\n",
      "what are bizarre sports that people play in washington dc?\n",
      "what religion are most people in pakistan?\n",
      "what are portuguese people considered?\n"
     ]
    }
   ],
   "source": [
    "find_related_questions('What are bizarre sports that people play in Washington DC?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
